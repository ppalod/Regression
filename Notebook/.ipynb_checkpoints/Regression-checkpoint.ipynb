{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "857deeb9",
   "metadata": {},
   "source": [
    "## Name: Prachi Ghanshyambhai Palod\n",
    "## USC ID: 1739359101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14173366",
   "metadata": {},
   "source": [
    "### Combined Cycle Power Plant Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ea380",
   "metadata": {},
   "source": [
    "### Functions used in the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7775b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_regression_coeff = {}\n",
    "\n",
    "def simple_linear_regression(feature):\n",
    "    X_feature = sm.add_constant(X[feature],prepend=False)\n",
    "    model = sm.OLS(y, X_feature)\n",
    "    results = model.fit()\n",
    "    print(results.summary(),'\\n')\n",
    "    simple_regression_coeff[feature] = results.params\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('PE')\n",
    "    plt.title('Linear Regression fit')\n",
    "    plt.scatter(X[feature],y,label='Data point')\n",
    "    plt.plot(X[feature],results.fittedvalues,color='red',label='Regression fit')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "def check_nonlinear_association(feature):\n",
    "    a = X[feature]\n",
    "    a = a[:,np.newaxis]\n",
    "    poly = PolynomialFeatures(degree=3)\n",
    "    a_poly = poly.fit_transform(a)\n",
    "    a_df=pd.DataFrame(a_poly)\n",
    "    a_df.columns=['Intercept','X','X^2','X^3']\n",
    "    X_non_linear = a_df\n",
    "    y_non_linear=df[df.columns[4]]\n",
    "    model = sm.OLS(y_non_linear, X_non_linear)\n",
    "    results = model.fit()\n",
    "    print(results.summary(),'\\n')\n",
    "    for i in range(1,4):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(X_non_linear.columns[i])\n",
    "        plt.ylabel('PE')\n",
    "        plt.scatter(X_non_linear[X_non_linear.columns[i]],y_non_linear,label='Data point')\n",
    "        plt.scatter(X_non_linear[X_non_linear.columns[i]],results.fittedvalues,color='red',label='Regression fit')\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "def check_pairwise_association():\n",
    "    a = X\n",
    "    poly = PolynomialFeatures(degree=2,interaction_only=True)\n",
    "    a_poly = poly.fit_transform(a)\n",
    "    a_df=pd.DataFrame(a_poly)\n",
    "    a_df.columns=['Intercept','AT','V','AP','RH','ATxV','ATxAP','ATxRH','VxAP', 'VxRH', 'APxRH']\n",
    "    X_non_linear = a_df\n",
    "    y_non_linear=df[df.columns[4]]\n",
    "    model = sm.OLS(y_non_linear, X_non_linear)\n",
    "    results = model.fit()\n",
    "    print(results.summary(),'\\n')\n",
    "    \n",
    "def quadratic_non_linearities(X):\n",
    "    a = X\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    a_poly = poly.fit_transform(a)\n",
    "    a_df=pd.DataFrame(a_poly)\n",
    "    a_df.columns=['Intercept','AT','V','AP','RH','AT^2','ATxV','ATxAP','ATxRH','V^2','VxAP', 'VxRH', 'AP^2','APxRH','RH^2']\n",
    "    return a_df\n",
    "\n",
    "def plot_error(result):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.grid(True)\n",
    "    plt.plot(result[\"k_inverse\"],result[\"train\"],color='red',label='Train error')\n",
    "    plt.plot(result[\"k_inverse\"],result[\"test\"],color='blue',label='Test error')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.xlabel('1/K values')\n",
    "    plt.ylabel('Mean squared error')\n",
    "    plt.title('Train and test error for different 1/K values')\n",
    "\n",
    "def best_estimates(result):\n",
    "    best_train_error = np.amin(result[\"train\"])\n",
    "    best_test_error = np.amin(result[\"test\"])\n",
    "    index_of_best_error = np.where(np.array(result[\"test\"]) == best_test_error)[0][0]\n",
    "    best_k_value = result[\"k_values\"][index_of_best_error]\n",
    "    print(\"Best train error:\",best_train_error)\n",
    "    print(\"Best test error:\",best_test_error)\n",
    "    print(\"Best k value (k*):\",best_k_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054492e",
   "metadata": {},
   "source": [
    "### (a) Download the Combined Cycle Power Plant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f77766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('../Data/CCPP/Folds5x2_pp.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01e43d",
   "metadata": {},
   "source": [
    "### (b) Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84011c",
   "metadata": {},
   "source": [
    "### i. How many rows are in this data set? How many columns? What do the rows and columns represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf64b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = len(df)\n",
    "cols = len(df.columns)\n",
    "print(\"The number of rows in the data set are:\",rows)\n",
    "print(\"The number of columns in the data set are:\",cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b57c96",
   "metadata": {},
   "source": [
    "<u>**Observation from the above results**</u>\n",
    "#### Rows\n",
    "Each row represents a data point of independent and dependent variables in the data set.\n",
    "#### Columns\n",
    "The first four columns: AT (Ambient Temperature), V(Exhaust Vacuum), AP(Ambient Pressure), RH(Relative Humidity) represent the predictors/independent variables and the last column: PE(net hourly electrical energy output) represent the response/dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8daa1e1",
   "metadata": {},
   "source": [
    "### ii. Make pairwise scatterplots of all the variables in the data set including the predictors (independent variables) with the dependent variable. Describe your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fbeee1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pair_plot = sns.pairplot(df);\n",
    "pair_plot.fig.suptitle(\"Relation among AT, V, AP, RH and PE\", y=1.05);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af29f68",
   "metadata": {},
   "source": [
    "<u>**Observation from the above results**</u>\n",
    "\n",
    "The observations drawn by analysing the above scatter plots are as follows:\n",
    "1. There is negative correlation between AT and PE.\n",
    "2. There is negative correlation between V and PE.\n",
    "3. There is positive correlation between AT and V."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350339dd",
   "metadata": {},
   "source": [
    "### iii. What are the mean, the median, range, first and third quartiles, and interquartile ranges of each of the variables in the dataset? Summarize them in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b6074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_df = {\n",
    "    'feature':[],\n",
    "    'mean':[],\n",
    "    'median':[],\n",
    "    'range':[],\n",
    "    'first_quartile':[],\n",
    "    'third_quartile':[],\n",
    "    'interquartile_range':[]\n",
    "}\n",
    "for col in df:\n",
    "    summary_df['feature'].append(col)\n",
    "    summary_df['mean'].append(df[col].mean())\n",
    "    summary_df['median'].append(df[col].median())\n",
    "    summary_df['range'].append(df[col].max()-df[col].min())\n",
    "    summary_df['first_quartile'].append(df[col].quantile(0.25))\n",
    "    summary_df['third_quartile'].append(df[col].quantile(0.75))\n",
    "    summary_df['interquartile_range'].append(df[col].quantile(0.75)-df[col].quantile(0.25))\n",
    "\n",
    "summary_df=pd.DataFrame(summary_df)\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ac7d1",
   "metadata": {},
   "source": [
    "### (c) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions. Are there any outliers that you would like to remove from your data for each of these regression tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec15a0",
   "metadata": {},
   "source": [
    "#### Splitting predictors and response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[0:4]]\n",
    "y = df[df.columns[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38222740",
   "metadata": {},
   "source": [
    "#### Simple linear regression model to predict the response for feature \"AT\" with a plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_linear_regression('AT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8624f",
   "metadata": {},
   "source": [
    "#### Simple linear regression model to predict the response for feature \"V\" with a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c39fac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simple_linear_regression('V')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602826a",
   "metadata": {},
   "source": [
    "#### Simple linear regression model to predict the response for feature \"AP\" with a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b689c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_linear_regression('AP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4a0db",
   "metadata": {},
   "source": [
    "#### Simple linear regression model to predict the response for feature \"RH\" with a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2185242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_linear_regression('RH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce13f2e",
   "metadata": {},
   "source": [
    "#### Detecting outliers from boxplot of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac8640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    sns.boxplot(X[X.columns[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0baa03",
   "metadata": {},
   "source": [
    "<u>**Observation from the above results**</u>\n",
    "\n",
    "1. There is a negative correlation between AT and PE. The R-squared value is high (i.e. 0.899) which says that our linear model fits the data well.\n",
    "2. There is a negative correlation between V and PE. The R-squared value is high (i.e. 0.757) which says that our linear model fits the data well.\n",
    "3. There is a little positive correlation between AP and PE. The R-squared value is low (i.e. 0.269) which says that our linear model does not fit the data well.\n",
    "4. There is a little positive correlation between RH and PE. The R-squared value is low (i.e. 0.152) which says that our linear model does not fit the data well.\n",
    "5. The p-value of AT, V, AP, RH is 0 which says there is a statistically significant association between the each predictor and the response.\n",
    "6. From the boxplots, we can see that there are outliers present in AP and RH which we would like to remove from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9bfbbf",
   "metadata": {},
   "source": [
    "### (d) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : Bj = 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921bcc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_feature_all = sm.add_constant(X,prepend=False)\n",
    "model = sm.OLS(y, X_feature_all)\n",
    "results = model.fit()\n",
    "print(results.summary(),'\\n')\n",
    "for i in range(0,4):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(X.columns[i])\n",
    "    plt.ylabel('PE')\n",
    "    plt.title('Multiple Regression fit')\n",
    "    plt.scatter(X[X.columns[i]],y,label='Data point')\n",
    "    plt.scatter(X[X.columns[i]],results.fittedvalues,color='red',label='Regression fit')\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc940a1d",
   "metadata": {},
   "source": [
    "<u>**Observation from the above results**</u>\n",
    "\n",
    "As observed from the above summary, p values for all the predictors AT, V, AP, RH is 0. Hence we reject the null hypothesis for all predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f3b13",
   "metadata": {},
   "source": [
    "### (e) How do your results from 1c compare to your results from 1d? Create a plot displaying the univariate regression coefficients from 1c on the x-axis, and the multiple regression coefficients from 1d on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b', 'c', 'y', 'r']\n",
    "plt.figure(figsize=(12,8))\n",
    "for i,feature in enumerate(X.columns):\n",
    "    plt.plot(simple_regression_coeff[feature][0],results.params[i],color=colors[i],marker='o',label=feature)\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Univariate vs Multiple Regression Coefficients')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Univariate Regression Coefficients')\n",
    "plt.ylabel('Multiple Regression Coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621850d",
   "metadata": {},
   "source": [
    "### (f) Is there evidence of nonlinear association between any of the predictors and the response?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff6808",
   "metadata": {},
   "source": [
    "#### Checking non-linear association between \"AT\" and \"PE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a5912",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_nonlinear_association('AT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba741a1d",
   "metadata": {},
   "source": [
    "#### Checking non-linear association between \"V\" and \"PE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nonlinear_association('V')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d7e0d",
   "metadata": {},
   "source": [
    "#### Checking non-linear association between \"AP\" and \"PE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5b9e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_nonlinear_association('AP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227bc07",
   "metadata": {},
   "source": [
    "#### Checking non-linear association between \"RH\" and \"PE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cdf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nonlinear_association('RH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44c4ce",
   "metadata": {},
   "source": [
    "<u>**Observation from the above results**</u>\n",
    "1. The p-values for the predictors AT, AP, RH is very low for all the terms (X, X^2, X^3) indicating nonlinear association between them and the response.\n",
    "2. The p-values for the predictor V is very low for the terms X, X^3 indicating nonlinear association between them and the response, but the p-value of the term X^2 is high indicating low significance between X^2 and the response.\n",
    "\n",
    "Overall p-values are low and we can say that there is a nonlinear association between all the predictors and response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd9125",
   "metadata": {},
   "source": [
    "### (g) Is there evidence of association of interactions of predictors with the response? To answer this question, run a full linear regression model with all pairwise interaction terms and state whether any interaction terms are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc8ca6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_pairwise_association()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44a720",
   "metadata": {},
   "source": [
    "<u>**Observation from the above results**</u>\n",
    "\n",
    "If we set the threshold of p-value to be 0.05, we can say that there is evidence of association of interations of predictors with the response of ATxV , ATxRH, VxAP, APxRH whose p-values are very low (less than 0.05). There is no association of interation for ATxAP whose p-value is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce3f0f",
   "metadata": {},
   "source": [
    "### (h) Can you improve your model using possible interaction terms or nonlinear associations between the predictors and response? Train the regression model on a randomly selected 70% subset of the data with all predictors. Also, run a regression model involving all possible interaction terms and quadratic nonlinearities, and remove insignificant variables using p-values (be careful about interaction terms). Test both models on the remaining points and report your train and test MSEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b0e2b",
   "metadata": {},
   "source": [
    "#### Splitting dataset into train and test sets with fixed seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49797daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88d5eb",
   "metadata": {},
   "source": [
    "#### Training the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321b989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_= sm.add_constant(X_train,prepend=False)\n",
    "model = sm.OLS(y_train, X_train_)\n",
    "results = model.fit()\n",
    "print(results.summary(),'\\n')\n",
    "for i in range(0,4):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(X_train_.columns[i])\n",
    "    plt.ylabel('PE')\n",
    "    plt.scatter(X_train_[X_train_.columns[i]],y_train,label='Data point')\n",
    "    plt.scatter(X_train_[X_train_.columns[i]],results.fittedvalues,color='red',label='Regression fit')\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fdb34",
   "metadata": {},
   "source": [
    "#### Train MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79865a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = results.predict(sm.add_constant(X_train,prepend=False))\n",
    "mean_squared_error(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f058ded",
   "metadata": {},
   "source": [
    "#### Test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89319d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = results.predict(sm.add_constant(X_test,prepend=False))\n",
    "mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bfa07",
   "metadata": {},
   "source": [
    "#### Run a regression model involving all possible interaction terms and quadratic nonlinearities, and remove insignificant variables using p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe441f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all_train = quadratic_non_linearities(X_train)\n",
    "X_all_test = quadratic_non_linearities(X_test)\n",
    "model = sm.OLS(list(y_train), X_all_train)\n",
    "results = model.fit()\n",
    "print(results.summary(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3ca35",
   "metadata": {},
   "source": [
    "#### Setting threshold for p value to be 0.05 and removing all the interation and quadratic terms in the above X_all_train dataset that has p value greater than 0.05 and re-running the model on X_new_train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_train = X_all_train.drop(columns=['ATxAP','V^2','VxAP','VxRH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08771ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(list(y_train), X_new_train)\n",
    "results = model.fit()\n",
    "print(results.summary(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568a661",
   "metadata": {},
   "source": [
    "#### Train MSE of new dataset of significant interaction terms and quadratic nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new_pred = results.predict(X_new_train)\n",
    "mean_squared_error(y_train, y_train_new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da051460",
   "metadata": {},
   "source": [
    "#### Removing insignificant terms from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064641c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_new_test = X_all_test.drop(columns=['ATxAP','V^2','VxAP','VxRH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7333b5",
   "metadata": {},
   "source": [
    "#### Test MSE of new dataset of significant interaction terms and quadratic nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_new_pred = results.predict(X_new_test)\n",
    "mean_squared_error(y_test, y_test_new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06251756",
   "metadata": {},
   "source": [
    "<u>**Observation from the above results**</u>\n",
    "\n",
    "If we set the threshold of p-value to be 0.05, we observe that the interation terms ATxAP, V^2, VxAP, VxRH have high p-values (more than 0.05). Hence, we drop these terms and train our model with rest of the significant terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449950b",
   "metadata": {},
   "source": [
    "### (i) KNN Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ae339",
   "metadata": {},
   "source": [
    "### i. Perform k-nearest neighbor regression for this dataset using both normalized and raw features. Find the value of k belongs to {1,2,....,100} that gives you the best t. Plot the train and test errors in terms of 1/k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a783ab",
   "metadata": {},
   "source": [
    "#### Normalizing features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_normalized_arr = min_max_scaler.fit_transform(X_train)\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized_arr,columns=['AT','V','AP','RH'])\n",
    "X_test_normalized_arr = min_max_scaler.fit_transform(X_test)\n",
    "X_test_normalized = pd.DataFrame(X_test_normalized_arr,columns=['AT','V','AP','RH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2c4e7",
   "metadata": {},
   "source": [
    "### k-nearest neighbor regression for raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fcf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_raw = {'k_values':[],'k_inverse':[],'train':[],'test':[]}\n",
    "for k in range (1,101):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = knn.predict(X_train)\n",
    "    y_test_pred = knn.predict(X_test)\n",
    "    \n",
    "    train_err = mean_squared_error(y_train,y_train_pred)\n",
    "    test_err = mean_squared_error(y_test,y_test_pred)\n",
    "    \n",
    "    error_raw['k_values'].append(k)\n",
    "    error_raw['k_inverse'].append(1/k)\n",
    "    error_raw['train'].append(train_err)\n",
    "    error_raw['test'].append(test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908c2fc",
   "metadata": {},
   "source": [
    "#### Best estimates for raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimates(error_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ebca3",
   "metadata": {},
   "source": [
    "#### Plot the train and test errors in terms of 1/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ec188",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_error(error_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd3270",
   "metadata": {},
   "source": [
    "### k-nearest neighbor regression for normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_normalized = {'k_values':[],'k_inverse':[],'train':[],'test':[]}\n",
    "for k in range (1,101):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train_normalized, y_train)\n",
    "    \n",
    "    y_train_pred = knn.predict(X_train_normalized)\n",
    "    y_test_pred = knn.predict(X_test_normalized)\n",
    "    \n",
    "    train_err = mean_squared_error(y_train,y_train_pred)\n",
    "    test_err = mean_squared_error(y_test,y_test_pred)\n",
    "    \n",
    "    error_normalized['k_values'].append(k)\n",
    "    error_normalized['k_inverse'].append(1/k)\n",
    "    error_normalized['train'].append(train_err)\n",
    "    error_normalized['test'].append(test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b5dd9",
   "metadata": {},
   "source": [
    "#### Best estimates for normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimates(error_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228b532",
   "metadata": {},
   "source": [
    "#### Plot the train and test errors in terms of 1/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error(error_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8763453",
   "metadata": {},
   "source": [
    "### (j) Compare the results of KNN Regression with the linear regression model that has the smallest test error and provide your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28d25a",
   "metadata": {},
   "source": [
    "1. The test MSE from linear regression model is 18.69 (from part (h)).\n",
    "2. The test MSE from KNN regression model for raw data is 15.73 (from part (i)).\n",
    "\n",
    "Hence we can say that, the test MSE obtained by KNN regressor is less than that obtained by the linear regressor. Thus for our data set, KNN regressor performs better than linear regressor.\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "- Our dataset contains less number of predictors and large number of data points.\n",
    "- Also, KNN regression is more flexible than Linear regression.\n",
    "- Thus KNN model fits our data well than linear model giving low train and test MSEs compared to linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06cdb5",
   "metadata": {},
   "source": [
    "### 2. ISLR: 2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfaae2c",
   "metadata": {},
   "source": [
    "### For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a96b80",
   "metadata": {},
   "source": [
    "(a) The sample size n is extremely large, and the number of predictors p is small. \n",
    "\n",
    "**Answer:** As sample size becomes large, the inflexible model underfits the data and as the flexibility of model increases it performs better. Also, flexible model won't overfit with large sample size and will result into low bias. Hence, in this case flexible model performs better.\n",
    "\n",
    "(b) The number of predictors p is extremely large, and the number of observations n is small.\n",
    "\n",
    "**Answer:** When the predictors size is extremely large and sample size is small, the flexible model will result into overfitting resulting into high variance. Hence, in this case inflexible model will perform better.\n",
    "\n",
    "(c) The relationship between the predictors and response is highly non-linear.\n",
    "\n",
    "**Answer:** It is hard to show non-linear relationship by inflexible model, thus we will have to use flexible model to find the non-linear relationship. Hence, in this case flexible model will perform better.\n",
    "\n",
    "(d) The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.\n",
    "\n",
    "**Answer:** High variance of error terms results in significant noise in our data. Thus a flexible model will result into large amount of noise.  Hence, in this case inflexible model will perform better\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73487ce",
   "metadata": {},
   "source": [
    "### 3. ISLR: 2.4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c33ea3",
   "metadata": {},
   "source": [
    "### The table provides a training data set containing six observations, three predictors, and one qualitative response variable. Suppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bca38",
   "metadata": {},
   "source": [
    "(a) Compute the Euclidean distance between each observation and the test point, X1 =X2 =X3 =0.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "The formula to calculate Euclidean distance in 3-dimension is sqrt((X1-X2)^2 + (Y1-Y2)^2 + (Z1-Z2)^2) . Given test point is (0,0,0). Hence the euclidean distance of test point from the given six observations is as follows:\n",
    "\n",
    "1. First point = sqrt((0-0)^2 + (0-3)^2 + (0-0)^2) = sqrt(9) = 3\n",
    "2. Second point = sqrt((0-2)^2 + (0-0)^2 + (0-0)^2) = sqrt(4) = 2\n",
    "3. Third point = sqrt((0-0)^2 + (0-1)^2 + (0-3)^2) = sqrt(10) = 3.16\n",
    "4. Fourth point = sqrt((0-0)^2 + (0-1)^2 + (0-2)^2) = sqrt(5) = 2.23\n",
    "5. Fifth point = sqrt((0-(-1))^2 + (0-0)^2 + (0-1)^2) = sqrt(2) = 1.41\n",
    "6. Sixth point = sqrt((0-1)^2 + (0-1)^2 + (0-1)^2) = sqrt(3) = 1.73\n",
    "\n",
    "(b) What is our prediction with K = 1? Why?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "From the distances calculated above, we can say that the nearest point when K = 1 would be 5th point and it's color is Green. Hence, our prediction would be green\n",
    "\n",
    "(c) What is our prediction with K = 3? Why?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "From the distances calculated above, we can say that the three nearest points when K = 3 would be 5th, 6th and 2nd point. The color of two points i.e. 2nd and 6th point is red and color of one point i.e. 5th point is green. Hence, our prediction will be red.\n",
    "\n",
    "d) If the Bayes decision boundary in this problem is highly non- linear, then would we expect the best value for K to be large or small? Why?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "If Bayes decision boundary is highly non-linear, that means our model has high flexibility. As K value decreases, flexibility of model increases. Hence, the best value for K would be small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da122745",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70104dc5",
   "metadata": {},
   "source": [
    "1. https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html\n",
    "2. https://www.statsmodels.org/devel/generated/statsmodels.regression.linear_model.OLS.html\n",
    "3. https://scikit-learn.org/stable/\n",
    "4. https://www.statsmodels.org/devel/examples/notebooks/generated/regression_plots.html\n",
    "5. https://www.statisticshowto.com/statistics-basics/find-outliers/\n",
    "6. https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e4310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
